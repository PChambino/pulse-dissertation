\chapter{Pulse: vital signs monitoring application} \label{chap:impl}

\section*{}

% Este capítulo deve começar por fazer uma apresentação detalhada do
% problema a resolver.

% Este capítulo pode ser dedicado à apresentação de detalhes de nível
% mais baixo relacionados com o enquadramento e implementação das
% soluções preconizadas no capítulo anterior.
% Note-se no entanto que detalhes desnecessários à compreensão do
% trabalho devem ser remetidos para anexos.

This chapter provides a detailed description of the problem addressed, defining
its scope, in section~\ref{sec:impl:problem}.

Moreover, section~\ref{sec:impl:details} details the implementation of the
work developed,
in order to create an Android application, named Pulse, for estimating a
person's heart rate based on the \evm{} method.

At the end, section~\ref{sec:impl:app} gives a description of resulting
Android application, Pulse.

\section{Problem description} \label{sec:impl:problem}

Section~\ref{sec:problem:evm} describes the main objective of the work which
consists of an implementation a video magnification method based on the
Eulerian perspective capable of running on a mobile device.

Then, section~\ref{sec:problem:heart} provides a description of a simple
application of the \evm{} method.

\subsection{Android-based implementation of \evm} \label{sec:problem:evm}

As stated on the previous chapters,
the \evm{} method is capable of magnifying small
motion and amplifying color variation which may be invisible to the naked eye.
Examples of the method application include:
estimation of a person's heart rate from the variation of its face's color;
respiratory rate from a person's chest movements;
and even, detect asymmetry in facial blood flow, which may be a symptom of
arterial problems.

The benefits of the Eulerian perspective is its low requirements for
computational resources and algorithm complexity, in comparison to other
attempts which rely on accurate motion estimation~\cite{Liu2005Motion}.
However, the existing limits of computational power on mobile devices
may not allow the \evm{} method to execute in real-time.

The main project's goal is to develop a lightweight, real-time \evm{}-based
method capable of executing on a mobile device, which will require
performance optimizations and trade-offs will have to taken into account.

\subsection{Vital signs monitoring} \label{sec:problem:heart}

As an objective to demonstrate that the \evm{}-based method developed is
working as expected, the creation of an Android application which
estimates a person's heart rate in real-time using the device's camera was
pursued.

This goal requires comprehension of the photo-plethysmography concept,
extraction of a frequency from a signal, and recognition / validation of a
signal as a cardiac pulse.

The application will then need to be tested in order to verify its estimations.
The test will be achieved by comparing results from a sphygmomanometer and
other existing application~\cite{Vitrox2013} which use a different
method to estimate a person's heart rate.

\section{Implementation details} \label{sec:impl:details}

Section~\ref{sec:impl:overview} provides an overview of the overall algorithm
implemented.

Then, various implementations of the \evm{} method are described on
section~\ref{sec:impl:evm}.

The face detection, signal validations, and heart rate estimation are
also detailed on sections~\ref{sec:impl:face},~\ref{sec:impl:validations},
and~\ref{sec:impl:estimation}, respectively.

Finally, section~\ref{sec:impl:android} details the interactions between
the Android platform and the implemented library.

\subsection{Overview} \label{sec:impl:overview}

\fig{algorithm-flow}{Overview of the implemented algorithm to obtain the heart
rate of a person from a webcam or video using the \evm{} method.}

In order to create an Android application capable of estimating a person's
heart rate, a desktop test application was first developed because of
its faster implementation speed, and easier testing.
The overall algorithm was divided into several steps, illustrated in
figure~\ref{fig:algorithm-flow}, which, later, was extracted into a
library, named Pulse, to be integrated into an Android application,
also named Pulse.
The language used to implement the desktop application and library
was C/C++. In addition, for the image processing operations, the
computer vision library, OpenCV, was used.

A short description of the overall algorithm's steps on
figure~\ref{fig:algorithm-flow} and application workflow is as follows:

\begin{enumerate}
  \item \emph{Original frame}, read frame from device's webcam;
  \item \emph{Face detector}, detect faces in current frame and match with
        previously detected faces in order to track multiple faces.
        Each face information is then fed into the following steps:
  \begin{enumerate}
    \item \emph{\evm}, magnify detected face's rectangle;
    \item \emph{Signal noise}, verify if the signal extracted from the
          current face is too noisy. If so, that face's signal is reset
          and marked as not valid;
    \item \emph{Detrending \& Normalization}, if the current face's signal is
          not too noisy, then detrend and normalize the signal in order to
          facilitate further operations with the signal;
    \item \emph{Validate signal}, the face's signal is then validated by
          verifying its shape and timing, in a similar but simpler manner as
          found in~\cite{Nenova2010Automated}. If the signal is given as
          invalid, it is kept as valid for a couple of time, because the
          validation algorithm may miss some peaks;
    \item \emph{Calculate beats per minute}, if the current face's signal is
          valid, it is then used to estimate the person's heart rate;
  \end{enumerate}
  \item \emph{Processed frame}, the resulting frame with each magnified face
        rectangle added back to the original frame.
\end{enumerate}

On the next sections, there are more detailed information and descriptions
of the algorithm's steps.

\subsection{\evm{} implementations} \label{sec:impl:evm}

\fig{evm-flow}{Overview of the \evm{} method steps.}

This section presents the details of several different implementations of the
\evm{} method.

The first implementations, described on sections~\ref{sec:impl:evm:gdownideal},
\ref{sec:impl:evm:gdowniir} and~\ref{sec:impl:evm:lpyriir},
were developed in Java to facilitate the integration into the Android
application. However, the OpenCV Java binding was still in its early stages
which ended up creating difficulties for the development. Thus, the final
implementation, on section~\ref{sec:impl:evm:final}, was implemented in C/C++,
which also reduces the number of JNI calls from the Android JVM and
increases the application performance.

The purpose of implementing multiple variants of the method was to study
how the method worked and select which spatial and temporal filters would
better fit the application goal: amplify color variation in real-time.

Figure~\ref{fig:evm-flow} shows generic steps of the method which will be
detailed on each of the following sections. The final step,
\emph{add to original frame}, however, remains the same in all
implementations. Which is when the magnified values are added back to the
original frame in order to obtain the processed frame.

\subsubsection{EvmGdownIdeal} \label{sec:impl:evm:gdownideal}

This was the first implementation, thus, its goal was to understand how the
method worked, and match the implementation provided, in MATLAB,
by~\cite{Wu2012Eulerian}. In addition, real-time support was implemented
by using a sliding window of $30$ frames.

\begin{description}
  \item[Resize down]\hfill\\
        This step applies a spatial filter by calculating a level of the
        Gaussian pyramid. This is achieved by looping to the desired
        level where the input to the next loop is the result from the previous
        loop, starting with the original frame. A Gaussian pyramid level is
        calculated by, first, convolving the input frame with the kernel, $K$:

        \begin{equation}
          K = \frac{1}{256}
          \begin{bmatrix}
             1 &  4 &  6 &  4 &  1 \\
             4 & 16 & 24 & 16 &  4 \\
             6 & 24 & 36 & 24 &  6 \\
             4 & 16 & 24 & 16 &  4 \\
             1 &  4 &  6 &  4 &  1 \\
          \end{bmatrix}
        \end{equation}

        and then, downsampling the frame by rejecting even rows and columns.

  \item[Temporal filter]\hfill\\
        An ideal bandpass filter was used to remove any amplification of
        undesired frequency from the color variation of each pixel.
        To construct this ideal filter, the Fourier transform was calculated
        for each pixel over the sliding window of $30$ frames. Then,
        frequencies below $45$ and above $240$ where set to zero, and the frame
        was rebuilt using the inverse Fourier transform.

  \item[Amplification]\hfill\\
        In this step, the result of the temporal filter is multiplied by an
        $\alpha$ value, which results in the magnification of the color
        variation selected by the temporal filter.

  \item[Resize up]\hfill\\
        This step performs the inverse operation of the \emph{resize down} step,
        where it upsamples the frame by inserting even rows and columns with
        zeros, and then, convolves the input frame with the same kernel
        multiplied by $4$. However, when the original frame is not multiple of
        two, an additional resize operation as to be done in order for the
        upsampled frame to match the original frame's size.
\end{description}

\subsubsection{EvmGdownIIR} \label{sec:impl:evm:gdowniir}

This implementation is very similar to the one above, but uses a different
temporal filter which does not require a sliding window of frames to support
real-time results. The filter used was an IIR bandpass filter, which was
constructed from the subtraction of two first-order lowpass IIR filters. Each
lowpass filter is computed as follows:

\begin{equation}
  L_n = L_{n-1} * (1 - \omega) + \omega * M
\end{equation}

where $M$ is the current frame, $L$ is the lowpass filter accumulator for
each frame, and $\omega$ is the cutoff frequency percentage.

The IIR temporal bandpass filter demonstrated similar results to the ideal
temporal filter used on the first implementation, without the need for
persisting a sliding window of frames, which simplifies the solution and reduces
the computational power required by the device.

\subsubsection{EvmLpyrIIR} \label{sec:impl:evm:lpyriir}

\fig{Lpyr}{Overview of image deconstruction and reconstruction for building a
Laplacian pyramid.}

Using the same IIR temporal filter as above, this implementation uses
a different spatial filter, which, instead of, computing a level of the
Gaussian pyramid, it constructs the full Laplacian pyramid and then applies
the temporal filter to each of its bands and each band is amplified differently.

\begin{description}
  \item[Resize down]\hfill\\
        Figure~\ref{fig:Lpyr} shows the steps to decompose and reconstruct
        an image for the purpose of building a Laplacian pyramid. The
        \emph{original image} must be decomposed into two images,
        \emph{blurred} and \emph{fine}, by applying any type of spatial lowpass
        filter and scaling the image down or up by 2. In this case, a
        Gaussian filter was applied as described on steps \emph{resize down}
        and \emph{resize up} of the first implementation. Further levels of the
        pyramid can be computed by decomposing the \emph{blurred image} in the
        same manner.

  \item[Temporal filter]\hfill\\
        The temporal filter used is the IIR bandpass filter, as described above
        for the previous implementation, only this time it is applied to each
        level of the pyramid.

  \item[Amplification]\hfill\\
        The amplification method in this implementation is more complex than
        the one previously used. It is based on the implementation provided
        by~\cite{Wu2012Eulerian}. It uses a different $\alpha$ value for each
        band of spatial frequencies, which corresponds to the Laplacian pyramid
        levels. The magnification value, $\alpha$, follows the equation:

        \begin{equation}
          (1 + \alpha) \delta(t) < \frac{\lambda}{8}
        \end{equation}

        where $\delta(t)$ represents the displacement function and $\lambda$
        the spatial wavelength. Further details about this equation may be
        found on~\cite[Section 3.2]{Wu2012Eulerian}.

  \item[Resize up]\hfill\\
        This step reconstructs the \emph{original image} by iteratively
        reconstructing each \emph{blurred image} until the now processed
        \emph{original image} is reached.
\end{description}

This implementation demonstrated that by constructing a Laplacian pyramid for
the spatial filter finer motion detail would be revealed, whereas the color
variation, the property to be analyzed, was less evident.

\subsubsection{Performance optimized EvmGdownIIR} \label{sec:impl:evm:final}

Because the OpenCV Java binding was not complete at the time, the Java desktop
implementations were not executing fast enough for real-time processing. Thus,
a C/C++ implementation of the method that demonstrated better color variation
was developed. In addition, after studying the performance of the application,
detailed on section~\ref{sec:results:perf}, the resize operations proved to be
computationally expensive. Therefore, the \emph{resize down} and
\emph{resize up} steps were modified to faster resize operations that did not
alter the resulting image.

\begin{description}
  \item[Resize down]\hfill\\
        This step was changed to a single resize operation using the
        OpenCV interpolation method named \emph{area}, which produces a similar
        result to the one provided by using a Gaussian filter and downsampling
        the image. However, instead of, iteratively downsampling the frame
        multiple times, it is now a single resize to a predefined size.

  \item[Resize up]\hfill\\
        This step was also modified to a single resize operation using the
        linear interpolation method, which produces a similar result to the
        one used previously where the image was upsampled iteratively using
        a Gaussian filter.
\end{description}

\subsection{Face detection} \label{sec:impl:face}

The \emph{face detection} step uses the OpenCV object detector initially
proposed on~\cite{Viola2001Rapid} and improved on~\cite{Lienhart2002Extended}.
Which has been previously trained to detect faces.

Because object detectors are computationally expensive, in order to
improve performance, a minimum size for the face detector was set to $40\%$
of the frame width and height. In addition, since it was expected for the
person to remain still during the reading, the face detector was set to execute
only once a second.

Face tracking is simply done by matching the previous and newly detected faces
to the closest one.
\begin{itemize}
  \item If there are less newly detected faces than the previously detected
        ones, then match each new face to the nearest older face. Any older
        face that is not matched with a newly detected face is marked for
        deletion. However, it is only deleted if it fails to find a match
        on the next time the face detector executes. This measure allows the
        face detector to miss a detection of a face one time.
  \item Otherwise, if the number of newly detected faces is equal or more than
        the older faces, then match each older face to the nearest newly
        detected face. Any newly detected face that is not matched with an
        older face is then marked as a new face.
\end{itemize}

Another performance boost was achieved by only magnifying each face
rectangle instead of the whole frame. Because of this, the face rectangle must
remain as still as possible, in order to introduce as few artifacts and noise
to the \evm{} method. To achieve this, the position and size of the face's
rectangle that is fed into the \evm{} method is interpolated between the
previous and newly detected faces, if the distance between the two, $d$,
is less than one third of the previous face's rectangle width, $w$:

\begin{equation}
  d < \frac{w}{3}
\end{equation}

And, the interpolation percentage, $r$, used is the ratio between these values:

\begin{equation}
  r = \frac{3d}{w}
\end{equation}

\subsection{Signal validation} \label{sec:impl:validations}

The signal validation has two phases. First, the \emph{raw} signal,
obtained by averaging the mean value of the green channel of
a rectangle's face, is checked for noise,
on step \emph{signal noise} in figure~\ref{fig:algorithm-flow}.
Then, on step \emph{validate signal}, the shape and timing of the detrended
and normalized signal is verified.

The raw signal noise is simply verified if the signal standard deviation
is higher than $50\%$ of the $\alpha$, amplification factor.

Then, each of the following operations are applied to the raw signal:
\begin{enumerate}
  \item \emph{detrend}, as in~\cite{Tarvainen2002Advanced}, also shortly
        described on section~\ref{sec:sota:post:detrend};
  \item \emph{normalization}, the normalized signal, $S'$, is obtained from
        the detrended signal, $S$, by subtracting the mean of the signal,
        $\bar{S}$, and dividing it by the signal standard deviation, $\sigma$:
        \begin{equation}
          S' = \frac{S - \bar{S}}{\sigma}
        \end{equation}
  \item \emph{mean filter}, is done by convolving the signal $3$ times with
        the kernel, $K$:
        \begin{equation}
          K = \frac{1}{5 * 5}
          \begin{bmatrix}
             1 & 1 & 1 & 1 & 1 \\
             1 & 1 & 1 & 1 & 1 \\
             1 & 1 & 1 & 1 & 1 \\
             1 & 1 & 1 & 1 & 1 \\
             1 & 1 & 1 & 1 & 1 \\
          \end{bmatrix}
        \end{equation}
\end{enumerate}

Finally, the shape and timing of signal is validated by a simplification of
the fast pulse wave detection algorithm~\cite{Nenova2010Automated}.
It identifies possible signal peaks by dividing the signal into consecutive
$200 ms$ time intervals and finding the absolute maximum. Some of these maximums
are rejected: if they are a boundary value of that segment; or if they fall
below a threshold, which is $60\%$ of the mean of the amplitude of the possible
identified peaks so far; or if the distance between two maximums is less than
or equal to $200 ms$, then the lower maximum is rejected.

\pagebreak

The signal is said to be valid if:
\begin{itemize}
  \item it includes at least two peaks;
  \item the peak count is between the valid human range, between 40 and 240 bpm;
  \item the peaks' amplitude standard deviation is less than $0.5$;
  \item the standard deviation of the time interval between peaks is less
        than $0.5$;
\end{itemize}

To prevent invalidating the signal if a peak is missed or miscounted,
the signal is kept as valid for the next 30 frames.

\subsection{Heart rate estimation} \label{sec:impl:estimation}

Even though the simplification of the fast pulse wave detection algorithm
presented on the previous section was capable of counting pulse wave peaks,
it would frequently miscount at least by one peak. Thus, since the period
analyzed was short, only 5 seconds at 20 frames per second, a miscounted peak
would introduce a large error to the final value.

In order to obtain the value of \emph{beats per minute} from the pulse signal,
the method presented on section~\ref{sec:sota:estimation:power} was used
every time the signal was marked as valid.
To prevent big fluctuations, the value was averaged over the values obtained
from the power spectrum method every 1 second.

\subsection{Android integration} \label{sec:impl:android}

\fig[p]{android-flow}{
  Overview of the interaction between the implemented Android application
  and library.
}

Due to performance, speed implementation, and easier testing, the algorithm's
library was implemented in C/C++. To integrate with the Android SDK, the
framework \emph{Java Native Interface} was used. Because each JNI call from the
Android SDK to the Android NDK introduces performance overhead,
the fewer calls made to the library, the faster the application executes.

The Android application workflow and interaction with the Pulse library is
illustrated on figure~\ref{fig:android-flow}. The arrows that cross the
modules' borders represent JNI methods' calls. As it is indicated, the whole
algorithm is executed in one JNI call each frame in order to improve
application performance. Three extra calls are made to obtain and display
data to the user.

\subsubsection{OpenCV for Android SDK} \label{sec:impl:android:opencv}

The OpenCV library is supported in the Android platform by installing
a separate Android application which deals with selecting the appropriate
OpenCV library for that device's architecture.

Because OpenCV for Android SDK was still in its early stage, it did not support
a couple of features which had to be implemented, such as, switching cameras at
runtime, portrait mode, stretching frame to container, removing the alpha
channel from the frame without introducing more operations.

\section{Pulse: Android application} \label{sec:impl:app}

\fig{pulse}{User interface of the implemented Android application, Pulse.}

The user interface of the implemented Android application, Pulse, is shown on
figure~\ref{fig:pulse}.

When the application is opened, the camera starts processing images and
requests the user to place its face in front of the camera. If a face is
detected the user is instructed to remain still in a bright place. Finally,
if a cardiac pulse is detected, the heart rate estimated and the signal
are shown on the top of the screen, while, the person's face magnification
is also shown.

Three buttons exists on the top-right corner. From left to right, the first
one, represented with a play icon, starts the record mode. The record mode
averages all the heart rate values estimated in that time period. Pressing the
button again finishes the record mode and displays the average of the
beats per minute. This mode was used for the procedure described on
section~\ref{sec:results:heart}.

The second button, represented with a camera icon, allows the application
to switch between the front and back camera. The third and last button,
represented with a wrench icon, opens a settings dialog, where the
\evm{} may be enabled and disabled and its amplification value, $\alpha$,
can be changed. Also, an option to turn on and off the number of
frames per second that the application is running at is available.

\section{Summary}

This chapter starts by describing the main goal and motivation for developing a
lightweight, real-time \evm{}-based method for the Android platform.

Then, an overview of the implemented algorithm's
steps is described.
The algorithm begins by detecting a person's face and magnifying it
using the \evm{} method. Then, it extracts a possible pulse signal by averaging
the green channel of a rectangle's face, which is validated, processed by
detrending and normalization, and validated again, by verifying its shape
and timing. Finally, the heart rate is estimated using the power spectrum
technique to obtain a signals frequency.

The chapter then details various implementations of the \evm{} method
which were implemented:
\begin{itemize}
  \item \emph{EvmGdownIdeal}, applies a spatial filter by computing a level
        of the Gaussian pyramid, and uses the \emph{Fourier transform} to
        implement a temporal ideal bandpass filter;
  \item \emph{EvmGdownIIR}, applies the same spatial filter as the previous,
        but uses a temporal IIR bandpass filter, which was constructed from
        the subtraction of two first-order lowpass IIR filters, which are more
        suitable for a real-time implementation;
  \item \emph{EvmLpyrIIR}, applies the same temporal filter as above, but
        computes a full Laplacian pyramid for the spatial filter, where each
        level is amplified differently;
  \item \emph{Performance optimized EvmGdownIIR}, is implemented in C/C++,
        while the others were implemented in Java, and the spatial filter
        is modified to single resize operations but the result is similar
        to computing a level of the Gaussian pyramid.
\end{itemize}

Face detection uses the OpenCV object detector module. In order to increase
performance, the face detector only executes every one second. Moreover,
only the face's rectangle is magnified, instead of the whole frame.

Two phases for validating the extracted signal exists. First, the raw
signal standard deviation is checked against a threshold. Then, the raw signal
is detrended and normalized, and the processed signal shape and timing is
verified by detecting its peaks.

The algorithm is integrated into an Android application by using
the JNI framework, so the Android SDK is capable of interacting with
the native code.

Finally, the resulting Android application, Pulse, user interface is described.
