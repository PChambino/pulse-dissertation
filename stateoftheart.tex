\chapter{State of the art} \label{chap:sota}

\section*{}

% Neste capítulo é descrito o estado da arte e são
% apresentados trabalhos relacionados para mostrar o que existe no
% mesmo domínio e quais os problemas em aberto.

% Deve deixar claro que existe uma oportunidade de desenvolvimento que
% cobre alguma falha concreta.

% O capítulo deve também efetuar uma revisão tecnológica às principais
% ferramentas utilizáveis no âmbito do projeto, justificando futuras
% escolhas.

% No final do capítulo deverá ser apresentado um resumo com as
% principais conclusões que se podem tirar.

This chapter focus on the heart rate estimation
from a person's face captured through a simple webcam.

Section~\ref{sec:sota:photo} describes the concept that explains how the
cardiac pulse is detected from a person's face in a remote, contact-free way.

Post-processing methods, which may be applied to the retrieved signal,
are detailed on section~\ref{sec:sota:post}.

In order to estimate the heart rate, some techniques are also detailed
on section~\ref{sec:sota:estimation}.

Finally, section~\ref{sec:sota:tech} reviews the main
technologies and tools used throughout this work.

\section{Photo-plethysmography} \label{sec:sota:photo}

Photo-plethysmography~(PPG) is the concept of measuring volumetric changes
of an organ optically. Its most established use is in pulse oximeters.

PPG is based on the principle that blood absorbs more light than surrounding
tissue thus variations on blood volume affect light
reflectance~\cite{Verkruysse2008Remote}.

The use of dedicated light sources and infra-red wavelengths, and contact probes
has been the norm~\cite{Ulyanov1993Pulse, Greneker1997Radar, Garbey2007Contact}.
However, recently, remote, non-contact PPG imaging has been
explored~\cite{Wieringa2005Contactless, Hu2008Feasibility}.

The method used on the article~\cite{Verkruysse2008Remote} captures the pixel
values (red, green, and blue channels) of the facial area of a previously
recorded video where volunteers were asked to minimize movements. The pixel
values within a region of interest (ROI) were then averaged for each frame.
This spatial averaging was found to significantly increase signal-to-noise
ratio. The heart rate estimation was then calculated by applying the
\emph{Fast Fourier transform} and the power spectrum as explained on
section~\ref{sec:sota:estimation:power}.

The authors of~\cite{Verkruysse2008Remote} demonstrate that
the green channel features a stronger cardiac signal as compared to the
red and blue channels. This is a strong evidence that the signal is due to
variations in the blood volume, because hemoglobin absorbs green light
better than red and blue light.

\section{Signal post-processing} \label{sec:sota:post}

After obtaining the raw pixel values (red, green, and blue channels), a
combination of the following methods may be used to extract and improve the
reflected plethysmography signal. However, each method introduces complexity
and expensive computation.

\subsection{\ica} \label{sec:sota:post:ica}

\ica{} is a special case of \emph{blind source separation} and is a
technique for uncovering independent signals from a set of observations
that are composed of linear mixtures of the underlying
sources~\cite{Comon1994Independent}.

In this case, the underlying source signal of interest is the cardiac pulse
that propagates throughout the body, which modifies the path length of the
incident ambient light due to volumetric changes in the facial blood vessels
during the cardiac cycle, such that subsequent changes in amount of reflected
light indicate the timing of cardiovascular events.

By recording a video of
the facial region, the red, green, and blue~(RGB) color sensors pick up a
mixture of the reflected plethysmographic signal along with other sources of
fluctuations in light due to artifacts. Each color sensor records a mixture
of the original source signals with slightly different weights. These observed
signals from the red, green and blue color sensors are denoted by $x_{1}(t)$,
$x_{2}(t)$ and $x_{3}(t)$ respectively, which are amplitudes of the recorded
signals at time point $t$. In conventional \ica{} model the number of
recoverable sources cannot exceed the number of observations, thus three
underlying source signals were assumed, represented by $s_{1}(t)$, $s_{2}(t)$
and $s_{3}(t)$. The \ica{} model assumes that the observed signals are linear
mixtures of the sources, i.e. $x_{i}(t) = \sum_{j=1}^{3} a_{ij} s_{j}(t)$ for
each $i=1,2,3$. This can be represented compactly by the mixing equation

\begin{equation}
  x(t) = As(t)
\end{equation}

where the column vectors $x(t) = [x_{1}(t), x_{2}(t), x_{3}(t)]^{T}$,
$s(t) = [s_{1}(t), s_{2}(t), s_{3}(t)]^{T}$ and the square $3\times3$
matrix $A$ contains the mixture coefficients $a_{ij}$. The aim of \ica{} model
is to find a separating or demixing matrix $W$ that is an approximation of the
inverse of the original mixing matrix $A$ whose output

\begin{equation}
  \hat{s}(t) = Wx(t)
\end{equation}

is an estimate of the vector $s(t)$ containing the underlying source signals.
To uncover the independent sources, W must maximize the non-Gaussianity of
each source. In practice, iterative methods are used to maximize or minimize
a given cost function that measures non-Gaussianity~\cite{Poh2010Non,
Poh2011Advancements}.

\subsection{\evm} \label{sec:sota:post:evm}

\fig{evm}{
  Overview of the \evm{} method. (source:~\cite{Wu2012Eulerian})
}

In contrast to the \ica{} model that focuses on extracting a single number,
the \evm{} uses localized spatial pooling and temporal filtering to extract
and reveal visually the signal corresponding to the cardiac pulse. This
allows for amplification and visualization of the heart rate signal at each
location on the face. This creates potential for monitoring and diagnostic
applications to medicine, i.e. the asymmetry in facial blood flow can be a
symptom of arterial problems.

Besides color amplification, the \evm{} method is also able to reveal
low-amplitude motion which may be hard or impossible for humans to see.
Previous attempts to unveil imperceptible motions in videos have been
made, such as, \cite{Liu2005Motion} which follows a \emph{Lagrangian}
perspective, as in fluid dynamics where the trajectory of particles
is tracked over time. By relying on accurate motion estimation and
additional techniques to produce good quality synthesis, such as,
motion segmentation and image in-painting, the algorithm complexity
and computation is expensive and difficult.

On the contrary, the \evm{} method is inspired by the \emph{Eulerian}
perspective, where properties of a voxel of fluid, such as pressure
and velocity, evolve over time. The approach of this method to motion
magnification is the exaggeration of motion by amplifying temporal
color changes at fixed positions, instead of, explicit estimation
of motion.

This method approach, illustrated in figure~\ref{fig:evm}, combines
spatial and temporal processing to emphasize subtle temporal changes
in a video. First, the video sequence is decomposed into different
spatial frequency bands. Because they may exhibit different
signal-to-noise ratios, they may be magnified differently.
In the general case, the full Laplacian pyramid~\cite{Burt1983Laplacian}
may be computed. Then, temporal processing is performed on each
spatial band. The temporal processing is uniform for all spatial
bands, and for all pixels within each band. After that, the extracted
bandpass signal is magnified by a factor of $\alpha$, which can be
specified by the user, and may be attenuated automatically. Finally,
the magnified signal is added to the original image and the spatial pyramid
collapsed to obtain the final output.

\subsubsection{Spatial filtering} \label{sec:sota:post:evm:spatial}

As mention before, the work of~\cite{Wu2012Eulerian} computes the full
Laplacian pyramid~\cite{Burt1983Laplacian} as a general case for spatial
filtering. Each layer of the pyramid may be magnified differently because
it may exhibit different signal-to-noise ratios, or contain spatial frequencies
for which the linear approximation used in motion magnification does not
hold~\cite[Section 3]{Wu2012Eulerian}.

Spatial filtering may also be used to significantly increase signal-to-noise
ratio, as previously mention on section~\ref{sec:sota:photo} and demonstrated
on the work of~\cite{Verkruysse2008Remote} and~\cite{Wu2012Eulerian}. Subtle
signals, such as, a person's heart rate from a video of its face, may be
enhanced this way. For this purpose the work of~\cite{Wu2012Eulerian} computes
a layer of the Gaussian pyramid which may be obtained by successively scaling
down the image by calculating the Gaussian average for each pixel.

However, for the signal of interest to be revealed, the spatial filter applied
must be large enough. Section 5 of~\cite{Wu2012Eulerian} provides an equation
to estimate the size for a spatial filter needed to reveal a signal at a
certain noise power level:

\begin{equation}
  S(\lambda) = S(r) = \sigma'^2 = k \frac{\sigma^2}{r^2}
\end{equation}

where $S(\lambda)$ represents the signal over spatial frequencies, and since
the wavelength, $\lambda$, cutoff of a spatial filter is proportional to its
radius, $r$, the signal may be represented as $S(r)$. The noise power,
$\sigma^2$, can be estimated using to the technique of~\cite{Liu2006Noise}.
Finally, because the filtered noise power level, $\sigma'^2$, is inversely
proportional to $r^2$, it is possible to solve the equation for $r$, where
$k$ is a constant that depends on the shape of the low pass filter.

\subsubsection{Temporal filtering} \label{sec:sota:post:evm:temporal}

\fig{temporal-filters}{
  Examples of temporal filters. (source:~\cite{Wu2012Eulerian})
}

Temporal filtering is used to extract the motions or signals to be amplified.
Thus, the filter choice is application dependent. For motion magnification,
a broad bandpass filter, such as, the butterworth filter, is preferred. A
narrow bandpass filter produces a more noise-free result for color
amplification of blood flow. An ideal bandpass filter is used
on~\cite{Wu2012Eulerian} due to its sharp cutoff frequencies. Alternatively,
for a real-time implementation low-order IIR filters can be useful for both:
color amplification and motion magnification. These filters are illustrated
on~\ref{fig:temporal-filters}.

\subsubsection{Emphasize color variations for human pulse} \label{sec:sota:evm:color}

\fig{evm-color}{
  Emphasis of face color changes using the \evm{} method.
  (source:~\cite{Wu2012Eulerian})
}

The extraction of a person's cardiac pulse using the \evm{} method
was demonstrated in~\cite{Wu2012Eulerian}. It was also presented that
using the right configuration can help extract the desired signal.
There are four steps to take when processing a video using the \evm{} method:

\begin{enumerate}
  \item select a temporal bandpass filter;
  \item select an amplification factor, $\alpha$;
  \item select a spatial frequency cutoff (specified by spatial wavelength,
        $\lambda_c$) beyond which an attenuated version of $\alpha$ is used;
  \item select the form of the attenuation for $\alpha$ —- either force
        $\alpha$ to zero for all $\lambda < \lambda_c$, or linearly scale
        $\alpha$ down to zero.
\end{enumerate}

For human pulse color variation, two temporal filters may be used, first
selecting frequencies within 0.4-4Hz, corresponding to 24-240 beats per
minute~(bpm), then a narrow band of 0.83-1Hz~(50-60 bpm) may be used,
if the extraction of the pulse rate was successful.

To emphasize the color change as much as possible, a large amplification
factor, $\alpha \approx 100$, and spatial frequency cutoff,
$\lambda_c \approx 1000$, is applied. With an attenuation of $\alpha$ to
zero for spatial wavelengths below $\lambda_c$.

The resulting output can be seen in figure~\ref{fig:evm-color}.

\subsection{Detrending} \label{sec:sota:post:detrend}

Detrending is a method of removing very large ultralow-frequency trends an
input signal without any magnitude distortion, acting as an high-pass filter.

The main advantage of the method presented on the work
of~\cite{Tarvainen2002Advanced}, compared to methods presented
in~\cite{Litvack1995Time} and~\cite{Porges1990Analysis}, is its simplicity.

The method consists of separating the input signal, $z$, into two components,
as $z = z_{stat} + z_{trend}$, where $z_{stat}$ is the nearly stationary
component, and $z_{trend}$ is the low frequency aperiodic trend component.

An estimation of the nearly stationary component, $\hat{z}_{stat}$, can
be obtained using the equation below. The detailed derivation of the equation
can be found in~\cite{Tarvainen2002Advanced}.

\begin{equation}
  \hat{z}_{stat} = (I - (I + \lambda^2 D_2^T D_2)^{-1}) z
\end{equation}

where $I$ is the identity matrix, $D_2$ is the discrete approximation of the
second order, and $\lambda$ is the regularization parameter.

\fig{detrend}{
  Original RR series and fitted trends (above) and detrended series without
  magnitude distortion (below). (source:~\cite{Tarvainen2002Advanced})
}

Figure~\ref{fig:detrend} presents an example of what this method is able to
achieve. The example, taken from the work of~\cite{Tarvainen2002Advanced}, uses
real RR series and the effect of the method on time and frequency domain
analysis of heart rate variability is demonstrated not to lose any useful
information.

\section{Heart rate estimation} \label{sec:sota:estimation}

In order to convert the extracted plethysmographic signal into the number
of beats per minute~(bpm), further processing must be done. Below two methods
capable of achieving this goal are highlighted.

\subsection{Power spectrum} \label{sec:sota:estimation:power}

\emph{Fourier transform} is a mathematical transform capable of converting
a function of time, $f(t)$, into a new function representing the frequency
domain of the original function.

To calculate the power spectrum, the resulting function from the
\emph{Fourier transform} is then multiplied by itself.

Since the values are captured from a video, sequence of frames, the function
of time is actually discrete, with a frequency rate equal to the video frame
rate, $FPS$.

The \emph{index}, $i$, corresponding to the maximum of the power spectrum can
then be converted into a frequency value, $F$, using the equation:

\begin{equation}
  F = \frac{i * FPS}{2 N}
\end{equation}

where $N$ is the size of the signal extracted. $F$ can then be multiplied by
$60$ to convert it to beats per minute, and have an estimation of the heart
rate from the extracted signal.

\subsection{Pulse wave detection} \label{sec:sota:estimation:pulse}

In~\cite{Nenova2010Automated}, an automated algorithm for fast pulse wave
detection is presented. The algorithm is capable of obtaining an
estimative of the heart rate from PPG signal, as an alternative to the power
spectrum described above. Moreover, it also introduces validation to the
waveform detection by verifying its shape and timing. Below a simplified
description of the algorithm is presented. A more detailed description can be
found in~\cite{Nenova2010Automated}.

\begin{enumerate}
  \item Identification of possible peaks and foots of individual pulses
  \begin{enumerate}
    \item Maximum ($MAX$)\\
          The signal is divided into consecutive $200 ms$ time intervals and
          for every segment the absolute maximum is determined. Some of these
          maximums are rejected: if they fall below a predetermined amplitude
          threshold; or if the distance between two maximums is less than or
          equal to $200 ms$, then the lower maximum is rejected.
    \item Minimum ($MIN$)\\
          The absolute minimum is determined between every two adjacent
          maximums. A minimum is rejected, if it is above a predetermined
          amplitude threshold. When a minimum is rejected, the lower-amplitude
          maximum of the two maximum adjacent to the rejected minimum is
          discarded too.
  \end{enumerate}

  \item Examination and verification of the rising edges
  \begin{enumerate}
    \item Validation of a single rising edge\\
          If a rising edge is rejected, its maximum and minimum are rejected.
          A rising edge is rejected, if its amplitude ($AMPL = MAX - MIN$) is
          lower than amplitude threshold; or its duration is lower than a
          threshold that depends on the sampling rate; or its amplitude does
          not increase smoothly.
    \item Estimation of the similarity of a rising edge to preceding and
          following rising edges accepted as valid\\
          Two rising edges are considered similar, if the amplitude of the
          lower-amplitude rising edge is greater than $50\%$ of the amplitude
          of the higher-amplitude rising edge; and if the maximum of the
          lower-amplitude rising edge is between $\pm60\%$ of the maximum of
          the higher-amplitude rising edge; and if the minimum of the
          lower-amplitude rising edge is between $\pm60\%$ of the minimum of
          the higher-amplitude rising edge; and if the duration of the shorter
          rising edge is greater then $33\%$ of the duration of the longer
          rising edge. The valid rising edges are then categorized according
          to its characteristics for the following step. The categorization
          description is suppressed for brevity and can be found
          at~\cite{Nenova2010Automated}.
    \item Verification of the current rising edge\\
          The rising edges categorized on the previous step are considered
          valid edges of a pulse wave if they fulfill at least one of the
          decision rules presented on~\cite{Nenova2010Automated} and suppressed
          for brevity.
  \end{enumerate}
\end{enumerate}

The validation process described here is important for discarding signals
which are not representative of pulse waves. Providing a way of calculating
the heart rate estimation only on valid pulse signals.

\section{Technologies} \label{sec:sota:tech}

Below two of the main technologies that will be used during this research
work are shortly described.

\subsection{Android SDK} \label{sec:sota:tech:android}

\emph{Android SDK} is the development kit for the \emph{Android}
platform. The \emph{Android} platform is an open source, Linux-based
operating system, primarily designed for touchscreen mobile devices,
such as, smartphones.

Because of its open source code and permissive licensing, it allows
the software to be freely modified and distributed. This has allowed
\emph{Android} to be the software of choice for technology companies
that require a low-cost, customizable, and lightweight operating system
for mobile devices and others.

\emph{Android} has also become the world's most widely used smartphone
platform with a worldwide smartphone market share of 75\%
during the third quarter of 2012~\cite{Idc2013Android}.

\emph{Android} consists of a kernel based on Linux kernel with middleware,
libraries and APIs written in C. Applications, usually, run on an application
framework which includes Java-compatible libraries based on
\emph{Apache Harmony}, an open source, free Java implementation.
\emph{Java bytecode} is then translated to run on the
\emph{Dalvik virtual machine}.

Porting existing Linux application or libraries to \emph{Android} is difficult
due to the lack of a native \emph{X Window System} and lack of support for
\emph{GNU} libraries. Support for simple C and SDL application is possible,
though, by the usage of \emph{JNI}, a programming framework that allows Java
code to call and be called by libraries written in C/C++.

\subsection{OpenCV -- Computer Vision Library} \label{sec:sota:tech:opencv}

\emph{OpenCV} is a library of programming functions mainly aimed at
real-time image processing. To support these, it also includes a
statistical machine learning library. Moreover, it is a cross-platform
and open source library that is free to use and modify under the BSD
license.

\begin{quote}
  ``OpenCV was built to provide a common infrastructure for computer
  vision applications and to accelerate the use of machine perception
  in the commercial products.''~\cite{Opencv2013About}
\end{quote}

\emph{OpenCV} is written in C/C++. There are binding for other languages,
such as, Python, Java, and even Android. However, Java and Android
implementation is recent and lacks features and stability.

\section{Summary}

This chapter starts by describing the concept behind the extraction of
cardiac pulse from a person's face captured through a simple video
or webcam.

It then presents several possible post-processing methods for improving
the extraction of the actual pulse signal. These methods include:
\begin{itemize}
  \item \emph{\ica}, a method capable of uncovering independent signals from
        a set of observations that are composed of linear mixtures of the
        underlying sources;
  \item \emph{\evm}, a method inspired by the \emph{Eulerian} perspective that
        exaggerates color variations by analyzing how each pixel value
        changes over time;
  \item \emph{Detrend}, a method which removes small trends from an input
        signal without distorting its amplitude.
\end{itemize}

Then algorithms for obtaining the actual beats per minute of the heart rate
from the signal are described:
\begin{itemize}
  \item \emph{Power spectrum}, a set of equations capable of finding the
        frequency of a signal using the \emph{Fourier transform};
  \item \emph{Pulse wave detection}, an algorithm for detecting and validating
        rising edges from a pulse signal.
\end{itemize}

Finally, important technologies for the work are described:
\begin{itemize}
  \item \emph{Android}, a Linux-based operating system, primarily designed for
        touchscreen mobile devices;
  \item \emph{OpenCV}, a \emph{Computer Vision} library of programming
        functions mainly aimed at real-time image processing.
\end{itemize}
